import pyodbc
import pandas as pd

# Establish connection to Hive using ODBC
conn = pyodbc.connect('DSN=Hive')  # Use your actual DSN configured for Hive

# SQL query to fetch the table data
query = "SELECT * FROM your_table_name"

# Execute the query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(query)

# Get column names and data types from the cursor description
columns = [column[0] for column in cursor.description]
column_types = [column[1] for column in cursor.description]

# Create a DataFrame with empty values
df = pd.DataFrame(columns=columns)

# Fetch all rows and append them to the DataFrame
for row in cursor.fetchall():
    df = df.append(dict(zip(columns, row)), ignore_index=True)

# Close the cursor
cursor.close()

# Map Hive data types to pandas data types
type_mapping = {
    'string': 'object',
    'varchar': 'object',
    'char': 'object',
    'boolean': 'bool',
    'tinyint': 'int64',
    'smallint': 'int64',
    'int': 'int64',
    'bigint': 'int64',
    'float': 'float64',
    'double': 'float64',
    'date': 'datetime64[ns]',
    'timestamp': 'datetime64[ns]',
    # Add more mappings as needed
}

# Apply the schema to the DataFrame
for column, hive_type in zip(columns, column_types):
    pandas_type = type_mapping.get(hive_type.lower(), 'object')
    df[column] = df[column].astype(pandas_type)

# Check the dtypes of the DataFrame after conversion
print("DataFrame dtypes after conversion:\n", df.dtypes)

# Close the connection
conn.close()

import pyodbc
import pandas as pd

# Establish connection to Hive using ODBC
conn = pyodbc.connect('DSN=Hive')  # Use your actual DSN configured for Hive

# SQL query to fetch the table data
query = "SELECT * FROM your_table_name"

# Execute the query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(query)

# Fetch all rows and column descriptions
rows = cursor.fetchall()
columns = [column[0] for column in cursor.description]
column_types = [column[1] for column in cursor.description]

# Create an empty DataFrame
df = pd.DataFrame(columns=columns)

# Append rows to DataFrame
for row in rows:
    df = df.append(dict(zip(columns, row)), ignore_index=True)

# Convert columns to appropriate pandas data types
for column, col_type in zip(columns, column_types):
    if 'int' in col_type:
        df[column] = df[column].astype('int64')
    elif 'float' in col_type:
        df[column] = df[column].astype('float64')
    elif 'bool' in col_type:
        df[column] = df[column].astype('bool')
    elif 'date' in col_type or 'time' in col_type:
        df[column] = pd.to_datetime(df[column])
    else:
        df[column] = df[column].astype('object')

# Check the dtypes of the DataFrame after conversion
print("DataFrame dtypes after conversion:\n", df.dtypes)

# Close the cursor and connection
cursor.close()
conn.close()

import pyodbc
import pandas as pd

# Establish connection to Hive using ODBC
conn = pyodbc.connect('DSN=Hive')  # Use your actual DSN configured for Hive

# SQL query to fetch the table data
query = "SELECT * FROM your_table_name"

# Execute the query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(query)

# Fetch all rows and column descriptions
rows = cursor.fetchall()
columns = [column[0] for column in cursor.description]
column_types = [column[1] for column in cursor.description]

# Close the cursor
cursor.close()

# Create an empty DataFrame
df = pd.DataFrame(columns=columns)

# Append rows to DataFrame
for row in rows:
    df = df.append(dict(zip(columns, row)), ignore_index=True)

# Convert columns to appropriate pandas data types based on Hive types
for column, col_type in zip(columns, column_types):
    if 'int' in col_type:
        df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype('int64')
    elif 'float' in col_type:
        df[column] = pd.to_numeric(df[column], errors='coerce').fillna(0).astype('float64')
    elif 'bool' in col_type:
        df[column] = df[column].astype('bool')
    elif 'date' in col_type or 'time' in col_type:
        df[column] = pd.to_datetime(df[column], errors='coerce')
    else:
        df[column] = df[column].astype('object')

# Check the dtypes of the DataFrame after conversion
print("DataFrame dtypes after conversion:\n", df.dtypes)

# Close the connection
conn.close()

import pyodbc
import pandas as pd

# Establish connection to Hive using pyodbc
conn = pyodbc.connect('DSN=YourHiveDSN', autocommit=True)

# SQL query to fetch the table data
query = "SELECT * FROM your_table_name"

# Execute the query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(query)

# Fetch all rows and column descriptions
rows = cursor.fetchall()
columns = [column[0] for column in cursor.description]
column_types = [column[1] for column in cursor.description]

# Close the cursor
cursor.close()

# Create an empty DataFrame with fetched data and columns
df = pd.DataFrame(rows, columns=columns)

# Convert columns to appropriate pandas data types based on Hive types
for i, col_type in enumerate(column_types):
    if 'int' in str(col_type):
        df[columns[i]] = pd.to_numeric(df[columns[i]], errors='coerce').fillna(0).astype('int64')
    elif 'float' in str(col_type):
        df[columns[i]] = pd.to_numeric(df[columns[i]], errors='coerce').fillna(0).astype('float64')
    elif 'bool' in str(col_type):
        df[columns[i]] = df[columns[i]].astype('bool')
    elif 'date' in str(col_type) or 'time' in str(col_type):
        df[columns[i]] = pd.to_datetime(df[columns[i]], errors='coerce')
    else:
        df[columns[i]] = df[columns[i]].astype('object')

# Check the dtypes of the DataFrame after conversion
print("DataFrame dtypes after conversion:\n", df.dtypes)

# Close the connection
conn.close()

import pyodbc
import pandas as pd

# Establish connection to Hive using pyodbc
conn = pyodbc.connect('DSN=YourHiveDSN', autocommit=True)

# Example: Original table name and filtered DataFrame
original_table_name = "your_original_table_name"
filtered_df = pd.DataFrame(...)  # Your filtered DataFrame

# SQL query to fetch the original table schema
schema_query = f"DESCRIBE {original_table_name}"

# Execute the schema query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(schema_query)

# Fetch column descriptions
columns = [column[0] for column in cursor.fetchall()]

# Close the cursor
cursor.close()

# Create a temporary table in Hive (using the original table's structure)
create_temp_table_query = f"CREATE TABLE temp_table_name LIKE {original_table_name}"
cursor = conn.cursor()
cursor.execute(create_temp_table_query)
cursor.close()

# Insert data from filtered DataFrame into the temporary table
cursor = conn.cursor()
for index, row in filtered_df.iterrows():
    insert_query = f"INSERT INTO temp_table_name ({', '.join(columns)}) VALUES ({', '.join(map(str, row.values))})"
    cursor.execute(insert_query)
cursor.close()

# Optionally, create a permanent table based on the temporary table
create_permanent_table_query = f"CREATE TABLE new_table_name AS SELECT * FROM temp_table_name"
cursor = conn.cursor()
cursor.execute(create_permanent_table_query)
cursor.close()

# Close the connection
conn.close()