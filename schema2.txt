import pyodbc
import pandas as pd

# Establish connection to Hive using ODBC
conn = pyodbc.connect('DSN=Hive')  # Use your actual DSN configured for Hive

# SQL query to fetch the table data
query = "SELECT * FROM your_table_name"

# Execute the query using pyodbc cursor
cursor = conn.cursor()
cursor.execute(query)

# Get column names and data types from the cursor description
columns = [column[0] for column in cursor.description]
column_types = [column[1] for column in cursor.description]

# Create a DataFrame with empty values
df = pd.DataFrame(columns=columns)

# Fetch all rows and append them to the DataFrame
for row in cursor.fetchall():
    df = df.append(dict(zip(columns, row)), ignore_index=True)

# Close the cursor
cursor.close()

# Map Hive data types to pandas data types
type_mapping = {
    'string': 'object',
    'varchar': 'object',
    'char': 'object',
    'boolean': 'bool',
    'tinyint': 'int64',
    'smallint': 'int64',
    'int': 'int64',
    'bigint': 'int64',
    'float': 'float64',
    'double': 'float64',
    'date': 'datetime64[ns]',
    'timestamp': 'datetime64[ns]',
    # Add more mappings as needed
}

# Apply the schema to the DataFrame
for column, hive_type in zip(columns, column_types):
    pandas_type = type_mapping.get(hive_type.lower(), 'object')
    df[column] = df[column].astype(pandas_type)

# Check the dtypes of the DataFrame after conversion
print("DataFrame dtypes after conversion:\n", df.dtypes)

# Close the connection
conn.close()